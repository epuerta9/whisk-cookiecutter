"""Langchain-based query handler."""
from whisk.kitchenai_sdk.kitchenai import KitchenAIApp
from whisk.kitchenai_sdk.schema import (
    WhiskQuerySchema,
    WhiskQueryBaseResponseSchema,
    TokenCountSchema,
    SourceNodeSchema
)
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.callbacks import get_openai_callback
from ..utils.logging import logger
from ..config import settings

class QueryHandler:
    """Langchain Query Handler for RAG-based question answering."""
    def __init__(self, kitchen: KitchenAIApp):
        self.kitchen = kitchen
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=settings.chroma_db_path,
            embedding_function=self.embeddings
        )
        self.llm = ChatOpenAI(temperature=0)
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}
            )
        )
        self._register_handlers()

    def _register_handlers(self):
        self.kitchen.query.handler("query")(self.handle_query)

    async def handle_query(self, data: WhiskQuerySchema) -> WhiskQueryBaseResponseSchema:
        """Handle RAG query using Langchain."""
        try:
            # Apply metadata filters if provided
            search_kwargs = {"k": 3}
            if data.metadata:
                search_kwargs["filter"] = data.metadata

            # Execute query and track token usage
            with get_openai_callback() as cb:
                response = self.qa_chain.run(data.query)

            # Create source nodes from retrieved documents
            retriever = self.vectorstore.as_retriever(search_kwargs=search_kwargs)
            docs = retriever.get_relevant_documents(data.query)
            source_nodes = [
                SourceNodeSchema(
                    text=doc.page_content,
                    metadata=doc.metadata,
                    score=1.0  # Langchain doesn't provide scores directly
                )
                for doc in docs
            ]

            # Create token counts
            token_counts = {
                "embedding_tokens": cb.prompt_tokens,  # Approximation
                "llm_prompt_tokens": cb.prompt_tokens,
                "llm_completion_tokens": cb.completion_tokens,
                "total_llm_tokens": cb.total_tokens
            }

            return WhiskQueryBaseResponseSchema(
                input=data.query,
                output=response,
                retrieval_context=source_nodes,
                token_counts=TokenCountSchema(**token_counts),
                metadata={"token_counts": token_counts, **data.metadata} if data.metadata else {"token_counts": token_counts}
            )
        except Exception as e:
            logger.error(f"Error in query handler: {str(e)}")
            raise 