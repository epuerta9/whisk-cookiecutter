"""Langchain-based storage handler."""
import tempfile
from pathlib import Path
from whisk.kitchenai_sdk.kitchenai import KitchenAIApp
from whisk.kitchenai_sdk.schema import (
    WhiskStorageSchema,
    WhiskStorageResponseSchema,
    TokenCountSchema
)
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.callbacks import get_openai_callback
from ..utils.logging import logger
from ..config import settings

class StorageHandler:
    """Langchain Storage Handler for document ingestion."""
    def __init__(self, kitchen: KitchenAIApp):
        self.kitchen = kitchen
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=settings.chroma_db_path,
            embedding_function=self.embeddings
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self._register_handlers()

    def _register_handlers(self):
        self.kitchen.storage.handler("storage")(self.handle_storage)
        self.kitchen.storage.on_delete("storage")(self.handle_delete)

    async def handle_storage(self, data: WhiskStorageSchema) -> WhiskStorageResponseSchema:
        """Handle document storage using Langchain."""
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_file_path = Path(temp_dir) / Path(data.name).name
                
                with open(temp_file_path, 'wb') as f:
                    f.write(data.data)
                
                loader = UnstructuredFileLoader(str(temp_file_path))
                documents = loader.load()
                
                # Add metadata to documents
                for doc in documents:
                    doc.metadata.update(data.metadata or {})
                
                # Split documents
                splits = self.text_splitter.split_documents(documents)
                
                # Store documents and track token usage
                with get_openai_callback() as cb:
                    self.vectorstore.add_documents(splits)

                token_counts = {
                    "embedding_tokens": cb.prompt_tokens,
                    "llm_prompt_tokens": 0,
                    "llm_completion_tokens": 0,
                    "total_llm_tokens": cb.total_tokens
                }

                return WhiskStorageResponseSchema(
                    id=data.id,
                    name=data.name,
                    label=data.label,
                    token_counts=TokenCountSchema(**token_counts),
                    metadata={"token_counts": token_counts, **data.metadata} if data.metadata else {"token_counts": token_counts}
                )
                
        except Exception as e:
            logger.error(f"Error in storage handler: {str(e)}")
            raise

    async def handle_delete(self, data: WhiskStorageSchema) -> None:
        """Handle document deletion."""
        try:
            # Delete documents with matching metadata
            self.vectorstore.delete(
                where={"source": data.id}
            )
            logger.info(f"Deleted storage for {data.id}")
        except Exception as e:
            logger.error(f"Error in delete handler: {str(e)}")
            raise 